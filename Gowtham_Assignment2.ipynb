{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gowtham Assignment2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gowtham66866/Inkers/blob/master/Gowtham_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nJ7YEw_vyjG",
        "colab_type": "text"
      },
      "source": [
        "# **Not an ideal network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGWv5hBhv2jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnMlDJQKv4VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dropout\n",
        "from keras.layers import Convolution2D, Dense\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgyOGCvppGSY",
        "colab_type": "text"
      },
      "source": [
        "# Here we are loading train and test mnist dataset. Keras API makes this available. Total of 70000 images are available here.60000 and 10000 are divided between train and test images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CdSu2lMwB9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkaL2q_qpXd_",
        "colab_type": "text"
      },
      "source": [
        "There are 60000 train images with each image dimensions of width 28, height 28 and channel number is a grayscale image. So this tensor is a 3 dimentional tensor of shape (60000, 28, 28).\"X_train\" is the  tensor used for all the train images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBZy0jZ8qB_y",
        "colab_type": "text"
      },
      "source": [
        "Matplotlib is a python librray used to display on of the images among the train images.The first image is printed which is a hand written digit 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLaDf0-rwCmj",
        "colab_type": "code",
        "outputId": "c062dbf9-6133-4dce-d5ea-bcdb2fd45763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[4])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7faa10cf6320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa8d5cb23c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBYVP7fsqcb7",
        "colab_type": "text"
      },
      "source": [
        "A 4 dimensional tensor data as input is taken by CNN model. The 1st dimension which is for applying CNN on multiple images in parallel is called a batch of images or batch size.Out of the 4 dimensions, 1st is for number of images, 2nd is for width of the image, 3rd is for height of the image, 4th is for number of channel of the image. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIxQWg4Tq1fl",
        "colab_type": "text"
      },
      "source": [
        "As per the requirement of CNN we have to modify the X_train and X_test from 3 dimensions to 4 dimensions, using reshape operation.Once the reshape is done, the image cannot be changed and code has to be rerun.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erb11jNwwFwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3UYaJ2Xre7_",
        "colab_type": "text"
      },
      "source": [
        "The operations performed here are\n",
        "\n",
        "1.   Input data precision is set to floating point 32 for better representation of input data and for better accuracy.\n",
        "2.  The normalisation between 0 to 1 is done by dividing each pixel of the image. The normalisation is done on pixel intensity which has a range between 0 to 255.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLK4YDoRwHet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ggQ0PGJsG7_",
        "colab_type": "text"
      },
      "source": [
        "y_train is the ouput target given for each of the 60000 mnist train data samples. In the below cell 10 of the target values are given to limit the set. We can observe that first value is a 5, which we saw earlier while printing the imput image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKLOmhlwJQl",
        "colab_type": "code",
        "outputId": "de5db085-7cf0-4d8d-c5f9-a5bb5fed2688",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8vxRjtMsiOW",
        "colab_type": "text"
      },
      "source": [
        "The expected neural network target format is the integer target values that are converted to a one-hot representation because for calculating catagorical cross-entropy . Since neural network predicts 10 probabilities for each image sample during forward propagation, the cross entropy is calculated by comparing it with target value for each of the 10 predicted value. Targeting one of the values and leaving all the other values, through loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YusMJguiwKsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAMWO3IEtvAH",
        "colab_type": "text"
      },
      "source": [
        "In the cell below, printing ten target values in one-hot representation after conversion as this is the way nueral network understand the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upxc99AswMW0",
        "colab_type": "code",
        "outputId": "a85a77f2-87d5-4fec-dd1c-9398904fd163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I1-ZHk_t9ky",
        "colab_type": "text"
      },
      "source": [
        "We have the actual model architecture in the cell below. We have three convolution layers flowed by a max pooling layer and then flowed by five convolution layers. Towards the end,we have 10 channels with dimenssions of 1x1 for each feature map. The flatten layer in the end makes the 10 channels as 10 output nodes. Applying softmax to the above will give us the prediction for the given input image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A50EfilMvHul",
        "colab_type": "text"
      },
      "source": [
        "Input shape of each image or the input channel dimension is (28, 28, 1)  with height, width and channel number respectively"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irTVUE47wNwr",
        "colab_type": "code",
        "outputId": "90c8883a-1b5e-4ab0-ae8d-1ca13857968c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1567
        }
      },
      "source": [
        "from keras.layers import Activation, MaxPooling2D\n",
        "\n",
        "model = Sequential()                                        #Global receptive field       | # output feature map\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1)))  # 3x        |       # (26,26,32)\n",
        "model.add(Convolution2D(64, 3, 3, activation='relu'))                         # 5x5       |       # (24,24,64)\n",
        "model.add(Convolution2D(128, 3, 3, activation='relu'))                        # 7x7       |       # (22,22,128)\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))                                     # 8x8       |      # (11,11,128)\n",
        "\n",
        "model.add(Convolution2D(256, 3, 3, activation='relu'))                        # 12x12     |      # (9,9,256)\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))                        # 16x16     |       # (7,7,512)\n",
        "model.add(Convolution2D(1024, 3, 3, activation='relu'))                       # 20x20     |       # (5,5,1024)\n",
        "model.add(Convolution2D(2048, 3, 3, activation='relu'))                       # 24x24     |       # (3,3,2048)\n",
        "model.add(Convolution2D(10, 3, 3, activation='relu'))                         # 28x28     |       # (1,1,10)\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 22, 22, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 9, 9, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 7, 7, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 5, 5, 1024)        4719616   \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 3, 3, 2048)        18876416  \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 1, 1, 10)          184330    \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 25,348,362\n",
            "Trainable params: 25,348,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (3, 3), activation=\"relu\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2048, (3, 3), activation=\"relu\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_17 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 22, 22, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 9, 9, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 7, 7, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 5, 5, 1024)        4719616   \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 3, 3, 2048)        18876416  \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 1, 1, 10)          184330    \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 25,348,362\n",
            "Trainable params: 25,348,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (3, 3), activation=\"relu\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2048, (3, 3), activation=\"relu\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a_o2NVXt7pk",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYZOpRb6yG7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O248wVQyMft",
        "colab_type": "code",
        "outputId": "91a34aa2-938d-4ea4-df0a-f69337d00c99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 116s 2ms/step - loss: 2.3027 - acc: 0.0986\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 115s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 115s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 115s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 115s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 115s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 115s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 115s 2ms/step - loss: 2.3026 - acc: 0.0987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7faa10d0d588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 115s 2ms/step - loss: 2.2696 - acc: 0.1155\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 3/10\n",
            "47040/60000 [======================>.......] - ETA: 24s - loss: 2.3026 - acc: 0.0985"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sst4KneiyOL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfJiXOKsyj4y",
        "colab_type": "code",
        "outputId": "4864e06f-6bcf-4e6c-d412-7885b350faec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.3025851249694824, 0.098]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwLSXt7nyn_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWKKoOKwyppN",
        "colab_type": "code",
        "outputId": "7d0add77-3787-4629-9cbc-7d6a61757200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbhg66VvQxw3",
        "colab_type": "text"
      },
      "source": [
        "**What is wrong with the above network?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brkPrA33Q434",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*  A fully connected feed forward layer as the last layer after last Convolution layer is needed.\n",
        "*   The network is has too many parameters for the given dataseta and this is leading to overfitting of the model.\n",
        "*   We don't need lot of channels to represent the features for a data set like MNIST. Channel number more than 128 is not needed.\n",
        "*   3 convolution layers before first max pooling are not needed since the image size is only 28x28 . Usually 1st max pooling is applied after the convolution layers have detected image to the level of edges. <br/> In the case of MNIST 28x28 images has one or two convolution layer before max pooling is enough to capture the edges.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imz7u-GAdIcR",
        "colab_type": "text"
      },
      "source": [
        "Making above changes is giving an accuracy of 99.68% on train and 99.5% evaluation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38JUkwFzYUAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}